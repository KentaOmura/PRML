
<span style="font-size: 150%;">**■決定理論**</span>
決定理論は、確率論と組み合わせる事によって、不確実性を含む状況における
最適な行動を決定する為の理論。

**■前段**
確率論の内容を知っている必要がある。
「確率論.md」を参照

**■本題**
決定理論でやりたい事は、確率論を使用して推論を実施した結果から
最適な行動の決定を行うようにしたい。

推論では、あるデータ($\boldsymbol{x}$)とそのデータに紐づく目的となるデータ($t$)の
同時確率分布$p(\boldsymbol{x}, t)$を訓練データから適切に算出する事を実施していた。
（→予測分布）
決定理論では、推論で算出した分布を使用して、ある一定の基準の基で決定を下す。
これを「決定」と呼ぶ。

以下の説明では、目的データ($t$)をあるクラス識別変数とする。
（連続ではなく、離散で考えるという事）

---

**■決定**
ある2つのクラス($C_1, C_2$)が存在するとする。
実施した事は、データ($\boldsymbol{x}$)を取得した時に、
そのデータからどちらのクラスを選択するべきかを決定したい。

この時、データ($\boldsymbol{x}$)を取得した時に$C_1, C_2$のそれぞれの確率は以下の式となる。

$p(C_k|\boldsymbol{x}) = \frac{p(\boldsymbol{x}|C_k)p(C_k)}{p(\boldsymbol{x})}$

上記の式はベイズの定理から以下のように解釈できる。
右辺の$p(C_k)$は各クラスの事前確率であり、
左辺は、入力データ$\boldsymbol{x}$を与える事で、$p(C_k)$をデータを基に修正している。
決定段階では、この式の左辺が最大になるクラスに割り当てれば良いと感覚的にわかる。

この感覚を数学的に抑えていく。
まず、データが取りうる値に対して1つのクラスを紐づけるようにする。（決定領域と呼ぶ）
異なるクラスの決定領域の間は決定境界と呼ばれる。

上記のような紐づけを行った後、
データが入力された時に、紐づけたクラス以外のクラスに割り当てるのは誤りとなる。
この誤りを最小にするように各$\boldsymbol{x}$に対応する決定領域を決定する事ができれば、次にデータが入った時に機械的にクラスを選ぶ事ができる。★

簡単の為、誤りの確率を最小にするのではなく、正解の確率を最大にする方向で考える。
2クラス分類において、正解の確率は以下の式となる。
$p(正解) = \sum_{k=1}^{2} p(\boldsymbol{x} \in R_k, C_k)$
$\hspace{35pt}=\sum_{k=1}^{2} \int_{R_k} p(\boldsymbol{x}, C_k)dx$

上記式より、正解の確率を最大にするには、$p(\boldsymbol{x}, C_k)$が最大となる
ように領域$R_k$を選択すれば良い。
ここで、同時確率分布に対して確率の乗法定理を使用すると、
$p(\boldsymbol{x}, C_k) = p(C_k|\boldsymbol{x})p(\boldsymbol{x})$とできる。
ここで、$p(\boldsymbol{x})$は各クラスで共通の為、$p(\boldsymbol{x}, C_k)$を最大にするには、$p(C_k|\boldsymbol{x})$を最大にすれば良い。
これで感覚ではなく、数学的に$p(C_k|\boldsymbol{x})$を最大にすれば良いことがわかった。

現実では、上記のような単純な計算で決定領域を求めるより複雑である。
その内容について期待損失の最小化について説明する

---

**■期待損失の最小化**

先のクラス分類問題では、推論フェーズで算出した予測確率を使用して
各クラスの予測確率を最大にするように各$\boldsymbol{x}$を決定領域に紐づけた。
この方法では、あるクラスを選択した事によるリスクが考慮されていない。

＝＝＝＝＝＝＝＝＝＝＝＝＝
どういう事かというと、
ガンかどうかをX線画像を取得して、決定したい時に、
ガンでないのに、ガンと診断すれば無駄な治療により体に身体的ダメージを与えたり、
また、ガン申告された事による精神的ストレスにさらされる可能性がある。
しかし、ガンなのにガンではないと決定してしまった時、本来受けるべき治療が受けれず
死ぬ可能性もある。
この場合、死の可能性は極力さける必要がある為、間違えてもガンと診断した方が良い。
＝＝＝＝＝＝＝＝＝＝＝＝＝

こういったリスクを考慮して「決定」を見直す。

ここではこのリスクを損失関数を使用して定式化する。
損失関数を定義する上で、損失行列を用意する。
先の2クラス分類問題で考えると、あるデータ$\boldsymbol{x}$を手に入れた時に本来
$C_1$に分類すべき所を$C_2$として分類してしまった時のペナルティの値を$L_{12}$とする。
同様に、$C_2$に分類すべき所を$C_1$として分類してしまった時のペナルティの値を$L_{21}$とする。（正解の分類はペナルティ0とする）
このように定義した時に、ペナルティの値は行列の形状となっており、
以下のような行列となる。

損失行列$L = \begin{bmatrix}
0 & L_{12} \\
L_{21} & 0
\end{bmatrix}$

この損失行列を使用して、クラス誤分類を最小化にする事が目的である。
しかし、損失関数は未知のクラスである真のクラスに依存している。
（真のクラスを知る事は不可能）

ここで与えられた入力ベクトル$\boldsymbol{x}$に対する、
真のクラスの不確実性の同時確率は$p(C_k|\boldsymbol{x})$で表現されるので
代わりに損失の平均を最小化する事にする。
式としては、以下の式となる。

$E[L] = \sum_{k} \sum_{j} \int_{R_j}L_{kj}p(\boldsymbol{x}, C_k)d\boldsymbol{x}$

各$\boldsymbol{x}$は決定領域$R_j$のどれかに独立に割り当てる事ができる。
実施したい事は、上記の式を最小化にする決定領域$R_j$を選ぶ事であり、
それは各$\boldsymbol{x}$毎に$\sum_{k}L_{kj}p(\boldsymbol{x}, C_k)$を最小化する事である。
（正しいクラスへ分類できれば最小化する事ができる。）
前の議論と同様に、同時確率分布を確率の乗法定理を用いて分解すると、
期待損失の式は以下となる。（決定領域$R_j$の期待損失）

$\sum_{/スにおいて$p(\boldsymbol{x})$は共通なので、
最終的には以下の式を最小にすればいい。

$\sum_{k}L_{kj}p(C_k|\boldsymbol{x})$

上記の式が最小になるように$\boldsymbol{x}$の領域を算出すればいい。

次に棄却について議論していく

---

**■棄却**

そもそもクラスの誤分類が発生するのは、事後確率が1より低い時に発生する。
（1より小さい時は、他のクラスの事後確率も値として存在するから。）
事後確率が1より低い領域はどのクラスに属するのか不確かな領域である。

なので、ある閾値を設ける事で、閾値より事後確率が小さい領域においては
決定を実施しないという戦略がある。
この戦略をとれば、棄却せず決定した場合においては、分類の誤差をより小さくする事ができる。
これを「棄却オプション」と呼ぶ。

上記を実現する為には、閾値を表現する$\theta$を導入し、
事後確率が$\theta$以下は決定を下さずに棄却をすれば実現できる。
→この閾値を1とすれば、全ての事例が棄却される。逆に$K$クラスあXる時に、
閾値を$\frac{1}{K}$とすれば、どの事例も棄却されない。

---

**■回帰における損失関数**

回帰問題の場合に立ち返って再び考える
決定段階では、各入力$\boldsymbol{x}$に対して、$t$の値に対する特定の推定値$y(\boldsymbol{x})$を選ぶ事からなる。
その際、損失として$L(t, y(\boldsymbol{x}))$をこうむるとする。
すると、期待損失は以下の式で記載できる。

$E[L]=\int \int L(t, y(\boldsymbol{x}))p(\boldsymbol{x},t)d\boldsymbol{\boldsymbol{x}}dt$

回帰問題では、よく使用される期待損失は二乗誤差$L(t, y(\boldsymbol{x})) = {(y(\boldsymbol{x})-t)^2}$である。
この場合、期待損失は以下の式で記載できる。

$E[L]=\int \int (y(\boldsymbol{x})-t)^2p(\boldsymbol{x},t)d\boldsymbol{\boldsymbol{x}}dt$

目標は上記の平均損失を最小にする$y(\boldsymbol{x})$を選ぶことである。
これには変分法を用いる。

>変分法とは
微分法は、変数を微小変化させた時の関数の変化の割合だった。
変分法は、関数を微小変化させた時の汎関数の変化の割合の事。
汎関数とは、関数を入力とする関数の事である。
通常の関数は入力は変数であった事に注意する。
ここから、変分法の定義を以下に記載する。
ここで、入力の関数を$y$とする汎関数$F[y]$を定義する。
入力の関数に対して、$\epsilon\eta(x)$分の微小変化を与える。
微小変化を与えた時のテイラー展開は以下の式となる。
$F[y+\epsilon\eta(x)]=F[y]+\epsilon\int \frac{\delta F}{\delta y}\eta(x)dx + O(\epsilon^2)$となる。
上記の式の第二項は、多変数関数の偏微分
$y(x_1+\epsilon_1, …,y_D+\epsilon_D) = y(x_1,…,x_D)+\sum_{i}^{D} \frac{\partial y}{\partial x_i}\epsilon_i + O(\epsilon^2)$の第二項の
無限次元版となっており、単純な拡張となっている。

期待損失が最小となる為の必要条件は、$y(x)$についての変分が0となる時であるので、
上記の変分法での定義から、
$\frac{\delta E}{\delta y}=2\int(y(\boldsymbol{x})-t)p(\boldsymbol{x},t)dt = 0$となるので、この式を$y(\boldsymbol{x})$について解く
