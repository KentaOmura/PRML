<span style="font-size: 150%;">**■情報理論**</span>
情報理論とは、情報と通信の数学的に論じる学問。
ここでは情報理論の視点で機械学習を論ずる。

**■本題**
始めに、離散確率変数を考え、
この変数に対して特定の値を観測した時にどれだけ情報論を受け取るかを考える。

滅多に観測されない値が観測された時は、情報量が多く。
頻繁に観測される値が観測された時は、情報量が少ない。
また、異なる分野の情報をそれぞれ取得した時の情報量は取得した情報の総計となる。
この事から、情報は以下のような数式で定義できる。
情報量を$h$、情報$x$が観測されたとして、
$h(x)=-\log p(x)$と定義する。
符号の$-$は確率が1より小さい事から、対数が負の値となる為、
情報量を正の値にする為の符号である。

次にこの情報量の定義を用いて、
送信する相手に平均的にどれくらいの情報量を渡すことができるかを計算する。
平均量を$H[x]$とすると、以下のような式となる。
$H[x]=-\sum_{x}p(x)\log p(x)$となる。
これをエントロピーと呼ぶ。

上記の情報量とエントロピーを機械学習に応用させていく。
（エントロピーの最大化等は省略する。）
その前に、連続変数の場合でのエントロピーを導出する。

---

今までは離散変数について考えてきた。
ここで、連続変数についても同様にエントロピーを考えたい。
$x$を等間隔に$\Delta x$で分ける。$p(x)$が連続であると仮定する。
また、$p(x)$の累積分布関数を$P(x)$とし、
始点を$\Delta i$とし、終点を$\Delta (i+1)$とすると、
平均値の定理が使用できる。
$\frac{P(\Delta (i+1)) - P(\Delta i)}{\Delta} = p(x_i)$
従って以下の式となる。
$\int_{\Delta i}^{\Delta (i+1)}p(x)dx = p(x_i)\Delta$

ここで、$i$番目の領域に属する$x$を$x_i$に割り当てる事によって量子化すると、
$x_i$の値を観測する確率は$p(x_i)\Delta$となる
従って、エントロピーは以下の式となる。
$-\sum p(x_i)\Delta\log p(x_i)\Delta$

上記の式を整理すると以下の式となる。($\sum p(x_i)\Delta = 1$が成立することを用いている。)
$-\sum p(x_i)\Delta\log p(x_i)\Delta = -\sum p(x_i)\Delta\log p(x_i) + \log \Delta$

さらに、$\log \Delta$を無視する事でエントロピーとして以下の式が得られる。
$-\sum p(x_i)\Delta\log p(x_i)$
上記の式において、$\Delta$を限りなく0に近づけると、総和は積分に変わるので、
$-\int p(x)\log p(x)dx$の式となる。
この式を微分エントロピーとなる。
この式を導出する過程で、$\log \Delta$を無視しているが、
この無視した項が連続変数の場合と、離散変数の場合での違いとなる。
$\log \Delta$の項で$\Delta$を0に近づけた時に、発散することから、
連続変数の場合は、無限ビットの情報量が必要なことがわかる。

以上が連続変数の場合における、エントロピーとなる。

---

次に、$x,y$が同時に観測された時の同時分布$p(x,y)$について考える。
$x$が観測された時に、$y$の値を特定する為の情報量は$-\log p(y|x)$である。
この時、$y$を特定する為に必要な情報量の平均は以下の式となる。
$H[y|x] = -\int \int p(y,x)\log p(y|x)dydx$

この式を$x$に対する$y$の条件付きエントロピーと呼ぶ。
ここで、確率の乗法定理を用いる
$H[y|x] = -\int \int p(y,x)\log p(y,x)dxdy + \int \int p(y,x)\log p(x)dydx = H[x,y] - H[x]$
従って、以下の式が得られる。
$H[x,y] = H[y|x] + H[x]$
これは、$x,y$を記述する為に必要な情報量の平均は、
$x$の情報量の平均と、$x$が与えられた条件下での$y$の情報量の平均との和で得られる。

---

ここから、この情報理論を機械学習に応用させる。
未知の分布$p(x)$を$q(x)$で近似するしたとする。
この近似した$q(x)$の情報量の平均と未知の分布の情報量の平均の差は以下の式となる。
$KL(p||q) = -\int p(x)\log q(x)dx - (-\int p(x)\log p(x)dx)$
$KL(p||q) = -\int p(x)\log \frac{q(x)}{p(x)}dx$
これをKLダイバージェンスと呼ぶ。

このKLダイバージェンスは凸関数(二階微分が正のため)なので、
イェンセンの不等式が利用できる。

イェンセンの不等式を適用すると以下の式となる。
$KL(p||q) = -\int p(x)\log \frac{q(x)}{p(x)}dx \geqq 0 $
よって、KLダイバージェンスの最小値は0である事がわかる。
また、0となる時は、$q(x)$が$p(x)$と一致したときに限る。

従って、$-\int p(x)\log q(x)dx$を最小にすることが$-\int p(x)\log p(x)dx$に近づける事となる。
この$-\int p(x)\log q(x)dx$は交差エントロピーと呼ばれ、機械学習においては
損失関数として利用される。

以上。
