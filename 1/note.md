###1章のまとめ

本章では、
機械学習の基礎となる確率論と決定理論、情報理論について解説されている。

ここでは簡単に内容についてまとめる。

<span style="font-size: 150%;">**■確率論**</span>
確率論は、不確実な事象に対して操作する枠組みを与える理論。
不確実なものとは何か？
→結果が決定していない、揺らぎのある事象の事。
　例えば、サイコロを振った後の目の数など。

**■前段**
確率の議論をする為に、確率空間を定義する。
確率空間とは？
$(Ω,F,p)$で表現される空間の事で、$Ω$は考えられる事象全体を表現し、
$p$はある事象が発生した時の面積を表現する。
$F$は難しいので省略する。一旦は$F$は性質の良い事象を集めた物と考える。
例えば、ルーレットを考える。
ルーレットを回した時に、針が数値と数値の中間で止まった時の事象などを省いた事象の集まりと考える。
$p$で求めている面積の事象も$F$の部分集合の面積を求めている。
（$Ω$はルーレットを回した時に針が中間で止まる事象も含められている。）
→ただし、基本は意識していない。厳密すぎるので・・・
　だから、ざっくり$Ω$と$F$は同じって考えてもいいと思う。数学警察に怒られそうだけど。。。

また、$p(Ω)=1$が成立する。

※確率を$p = \frac{事象の数}{全体事象の総和}$で表現されるが、これは大数の法則から表現される。

次に確率変数を導入する。
確率変数とは、ある事象に対して、ある値を返す関数の事である。
確率変数を定義する事で、現実世界の事象を自分で定めた数値と紐づける事で、
扱いが便利になる。

**■本題**
★重要なのは以下の定理（何故この定理が成立するかは面積で考えると分かりやすい）
・確率の乗法定理
&emsp;$p(x,y) = p(x|y)p(y)$
・確率の加法定理
&emsp;$p(x) = \sum_{n=1}p(x,y_n)$

上記の定理から以下の関係式が得られる
&emsp;$p(x|y) =$ <span style="font-size: 150%;">$\frac{p(x,y)}{p(y)}$</span>

$p(x,y) = p(y,x)$の関係から上記の関係式のyとxを入れ替え、
同時確率については、確率の乗法定理を使用する事でベイズの定理が導出できる。

&emsp;$p(y|x) =$ <span style="font-size: 150%;">$\frac{p(x|y)p(y)}{p(x)}$</span>

上記の意味は、
$y$の分布$p(y)$がデータ$x$の情報を得る事で、$x$を考慮した上で
$y$の分布を更新するという意味になる。

最後に、独立性について述べる。
・独立性
2つの確率変数を考えた時に、それぞれが独立した確率変数の場合
即ち、それぞれの確率変数が関係性をもっていない時。
（1つの観測値を見ても、もう1つの確率に変わりがないという事）
よって以下の式が成立する。
&emsp;$p(x|y) = p(x)$

上記式より、2つの確率変数が独立の時に限り、確率の乗法定理は以下となる。
&emsp;$p(x,y) = p(x)p(y)$

---
**●指標**
不確実な事象を取り扱う上で、一定の指標を得たい時がある。
そこで、指標として、期待値と分散、共分散について考える。

・期待値
期待値とは、確率変数の取りうる平均値を表現する指標。
イメージとしては、確率を面積、確率変数を高さと考えて体積を求めているとイメージする。
確率の総和は1になるので、面積の総和は1になる事を踏まえると、
各体積の総和は、面積1の高さがある一定の値を持つ1つの立方体になる。
この高さを期待値と呼ぶ。または、平均とも呼ぶ。
式
$E[x] \equiv \sum_{n=1}x_np(x_n)$

・分散
分散とは、観測され得る確率変数が、期待値からどれだけ離れているのかを表現する指標。
離れている事を、期待値から確率変数を引く事で表現する。
引いた後の値はもちろん確率変数となる。導出過程で確率変数が出ているから。
なので、一定の評価を得る為に、期待値を取る。
しかし、そうすると、期待値から確率変数を引いた時に負の数も現れるので、
正と負で打消し合い正しい分散値が取得できない。
その為、期待値から確率変数を引く時に、二乗を取る。
（絶対値という手もあるが、今後の計算を考えると二乗の方が都合がいい。）
よって以下の式で分散を導出できる。
$V[x] \equiv E[(x-E[x])^2]$

・共分散
共分散とは、観測され得る異なる2つの確率変数の関係性を表現する為の指標。
★分散と名前が似ているけど、得られる意味は異なっているので注意★
2つの確率変数をそれぞれ$x, y$と置き、それぞれの平均値を$μ, ν$と置く。
その上で共分散を以下のように定義する。
$Cov[x,y] \equiv E[(x-μ)(y-ν)]$
意味としては、$x$が大きい(小さい)と$y$も大きく(小さく)なる傾向がある事や、
$x$が大きく(小さく)ても$y$が大きい（小さい）の傾向が無い事を知る事ができる。

>補足
>期待値と分散の式で()ではなく、[]を使用しているのは、関数ではなく汎関数の為。
>関数は入力として数を入れて、数を出力する物。
>汎関数は入力として関数を入れて、数を出力する物。
>確率変数は関数だから、期待値と分散、共分散は汎関数。
---

ここで、上記の指標の関係式を考える。
・期待値
確率変数を定数倍した時の期待値の関係式は以下の通り。
$E[ax] = aE[x]$
>理由
>左辺は確率変数をa倍した状態で各体積を足す事を意味しており、
>右辺は体積を求めた後a倍する事を意味しているので、同じ意味となっている。

確率変数に定数を加えた時の期待値の関係式は以下の通り。
$E[x+a] = a+E[x]$
>理由
>左辺は確率変数にaを加えて各体積を足す事を意味しており、
>右辺は各体積の総和を求めた後、aを足す事を意味しているので、同じ意味となっている。

2つの（独立な）確率変数の和の期待値の関係式は以下の通り。
$E[x+y] = E[x] + E[y]$
>理由
>左辺は2つの確率変数を足した後、各体積を足す事を意味しており、
>右辺はそれぞれの確率変数の体積を求めた後で、足し合わせているので同じ意味となっている。

2つの独立な確率変数の積の期待値の関係式は以下の通り。
$E[xy] = E[x]E[y]$
>理由
>$E[xy] = \sum_{i}\sum_{j}x_iy_jp(x=x_i,y=y_j)$と記載する事ができ
>確率の乗法定理から以下の式に変換できる。
>$E[xy] = \sum_{i}x_ip(x=x_i)\sum_{j}y_jp(y=y_j)$と記載する事ができる。
>すると、右辺は各確率変数の期待値の積になっている事がわかる。
>よって、$E[xy] = E[x]E[y]$
>※確率変数が独立でない場合については特に性質の良い関係式は得られない。

・分散
分散の定義から別の表現を行う。その関係式は以下の通り。
$V[x] = E[x^2] - E[x]^2$
>理由
>左辺の式は定義から、$E[x]=μ$と置くと、$V[x]=E[(x-μ)^2]$
>ここで、$E[(x-μ)^2]$の中身を展開する。
>すると、$E[(x^2-2xμ+μ^2)]$となり、期待値の和の関係性、定数倍の関係性から
>$E[(x^2)]-2μE[x]+μ^2$となる。ここで、$E[x]=μ$の関係式から、
>$V[x] = E[x^2] - E[x]^2$となる。

確率変数を定数倍した時の分散の関係式は以下の通り。
$V[ax] = a^2V[x]$
>理由
>左辺の式は定義から、$E[x]=μ$と置くと、$V[ax]=E[(ax-aμ)^2]$
>ここで、$E[(ax-aμ)^2]$について注目する。
>aは共通しているので、括りだすと$E[a^2(x-μ)^2]$が得られる。<br>よって、期待値の関係式から$a^2E[(x-μ)^2]$となる。
>よって、$V[ax] = a^2V[x]$

確率変数に定数を加えた時の分散の関係式は以下の通り。
$V[x+a] = V[x]$
>理由
>左辺の式は定義から、$E[x]=μ$と置くと、$V[a+x]=E[(a+x-a-μ)^2]$
>よって、右辺は$E[(x-μ)^2]$となり、$V[x]$そのものとなっている。

2つの独立な確率変数の和の分散の関係式は以下の通り。
$V[x+y] = V[x]+V[y]$
>理由
>左辺の式は定義から、$E[x]=μ, E[y] = ν$と置くと、<br>$V[x+y]=E[(x+y-μ-ν)^2]$
>ここで、$E[(x+y-μ-ν)^2]$について注目する。
>中身を展開すると、$(x-μ)^2+2(x-μ)(y-ν)+(y-ν)^2$となる。<br>ここで期待値の和の関係式から以下の式なる。
>$E[(x-μ)^2]+E[2(x-μ)(y-ν)]+E[(y-ν)^2]$となる。<br>ここで$E[2(x-μ)(y-ν)]$について着目する。
>式から、これは共分散の定義となっている。共分散は独立の時に0となるので、真ん中の項は消える。
>従って、$E[(x-μ)^2]+E[(y-ν)^2]$となり、これは分散の定義式となっているので、
>$V[x+y] = V[x]+V[y]$が得られる。

・共分散
共分散の定義から別の表現を行う。その関係式は以下の通り。
$Cov[x,y] = E[xy]-E[x]E[y]$
>理由
>左辺の式は定義から、$E[x]=μ, E[y] = ν$と置くと、<br>$Cov[x,y]=E[(x-μ)(y-ν)]$
>ここで、$E[(x-μ)(y-ν)]$の中身を展開する。
>すると、$E[(xy-xν-μy+μν)]$となり、期待値の和の関係性、定数倍の関係性から
>$E[xy]-νE[x]-μE[y]+μν$となる。ここで、$E[x]=μ, E[y] = ν$の関係式から、
>$E[xy]-E[x]E[y]$となる。

2つの確率変数をそれぞれ定数倍した時の共分散の関係式は以下の通り。
$Cov[ax,by] = abCov[x,y]$
>理由
>左辺の式は定義から、$E[x]=μ, E[y] = ν$と置くと、<br>$Cov[ax,by]=E[axby]-E[ax]E[by]$
>期待値の和と定数倍の関係式から、
>$Cov[ax,by]=abE[xy]-abE[x]E[y]$となる。abで括ると、以下の式となる。
>$Cov[ax,by] = abCov[x,y]$

2つの確率変数にそれぞれ定数を加えた時の共分散の関係式は以下の通り。
$Cov[a+x,b+y] = Cov[x,y]$
>理由
>左辺の式は定義から、$E[x]=μ, E[y] = ν$と置くと、<br>$Cov[a+x,b+y]=E[(a+x)(b+y)]-E[a+x]E[b+y]$
>$E[(a+x)(b+y)]$の中身を展開すると、$E[ab+ay+xb+xy]$となる。
>期待値の積と和及び、定数和の関係式から以下の式となる。
>$Cov[a+x,b+y]=ab+aE[y]+bE[x]+E[xy]-(a+E[x])(b+E[y])$<br>整理すると、
>$Cov[a+x,b+y]=E[xy]-E[x]E[y]$となり、右辺は共分散の定義となっている。

2つ確率変数が独立の時の共分散の関係式は以下の通り。
$Cov[x,y] = 0$
>理由
>左辺の式は定義から、$E[x]=μ, E[y] = ν$と置くと、<br>$Cov[x,y]=E[xy]-E[x]E[y]$
>x,yが独立の時は、期待値の積の関係式から、
>$Cov[x,y]=E[x]E[y]-E[x]E[y]$となる。従って、右辺は0となる。

---
**●確率密度と確率分布**
今までお話は確率変数が離散の場合について考えてきた。
確率変数が連続の時は、上記の指標や確率の乗法定理、和の定理は単純に
$\sum_{}$から$\int$に変更すれば良い。
但し、確率変数xが離散の時に確率を表現していた$p(x)$についての解釈は変えなければいけない。
離散から連続になると、考えられる場合の数は有限から無限に変わる。
その為、確率の定義により、特定の事象の面積は0となる。ある事象を$x$と置くと、$p(x)=0$。
しかし、$\sum_{n}p(x_n)=1$は成立する。
この事から、1つ1つの事象の確率は0なのに、総和したら1となる。ちょっとした矛盾が発生する。
連続の場合は、事象を微小区間でまとめる事（量子化）で、この矛盾を解決する。
この微小区間を$Δx$とすると、確率を$p(x)Δx$で定義する。この時、$p(x)$を確率密度関数と呼ぶ。
何故確率密度関数と呼ぶかというと、$Δx$は距離を表しており、$p(x)Δx$を確率（面積）とするなら
$p(x)$は密度を表現している事になるから。
全体の確率(面積)$P(x)$は$P(x) = \int_{-∞}^{∞}p(x)dx$となる。
左辺を$x$について微分すると、$\frac{dP(x)}{dx} = p(x)$となる。

確率密度関数は確率の定義より以下の関係式を満たす。
・$\int_{-∞}^{∞}p(x)dx$ = 1
・$p(x) \geqq 0$

次に、確率分布について簡単に述べる。
確率分布とは、事象が起きる確率を表現した分布の事。（そのまんま～）
離散の場合で、サイコロの確率分布を以下に記載する。
|     | P(x) | 
| --- | :--- | 
| 1   | 1/6  | 
| 2   | 1/6  | 
| 3   | 1/6  | 
| 4   | 1/6  | 
| 5   | 1/6  | 
| 6   | 1/6  | 
上記のように、考えられる事象についての確率を右に並べた表を分布と呼ぶ。
では、連続な確率変数の場合はどうなるのか。
上記のように考えると、すべての確率は0になるので、ここでも確率密度を使用して分布を考える。
以下に確率密度として、ガウス分布を使用した分布を記載する。
```python
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-10,10,0.1)
# 標準正規分布
y = 1/(np.sqrt(2*np.pi*1)) * np.exp(-1/2*np.power(x-0,2))

plt.plot(x,y)
plt.show()
```
![ガウス分布](C:/work/数学/Prml上/PRML/PRML/1/ガウス分布.png) 

---
**●ガウス分布**
ここで確率分布で最も重要と言っても過言ではない分布であるガウス分布を紹介する。
ガウス分布は解析的に性質が良いのと、あらゆるデータの分布がこのガウス分布に従っている事から
機械学習では頻繁に使用される。

ガウス分布は以下のように定義される。
$N(x|μ,σ^2) = \frac{1}{\sqrt(2πσ^2)}\exp(-\frac{(x-μ)^2}{2σ^2})$
上記で注目する箇所は$\exp(-\frac{(x-μ)^2}{2σ^2})$。
係数部分は分布の総和を1にする為の規格化係数だから。
この時、ガウス分布のパラメータである$μ$を平均、$σ^2$を分散と呼び、$σ$を標準偏差と呼ぶ。

規格化係数の確認。
規格化係数の総和を省いた形で計算を行う
$\int_{-∞}^{∞}\exp(-\frac{(x-μ)^2}{2σ^2})dx$
上記において、$(x-μ)=z$と置く。
$(x-μ)=z$の両辺を$x$で微分すると$dz = dx$となる。
また、$\lim_{x \to \infty}z =  \infty$であり、$\lim_{x \to -\infty}z =  -\infty$。
よって、上記の式は以下の式で記載しなおす事ができる。
$\int_{-∞}^{∞}\exp(-\frac{z^2}{2σ^2})dz$
これは、ガウス積分の形になっているので、積分すると、$\sqrt(2\piσ^2)$
よって、規格化する為には、$\frac{1}{\sqrt(2\piσ^2)}$を係数とすれば良い。

ここからは、ガウス分布の性質について確認していく。


ガウス分布の期待値はどうなる？
定義より、
$E[x] = \int_{-∞}^{∞}\frac{1}{\sqrt(2πσ^2)}\exp(-\frac{(x-μ)^2}{2σ^2})xdx$と記載できる。
$\frac{(x-μ)}{\sigma}=z$と置き、両辺の$x$についての微分を取ると$\frac{1}{\sigma}dx = dz$となる。
ここで、与式を以下のように変形する。
$E[x] = \int_{-∞}^{∞}\frac{1}{\sqrt(2π\sigma^2)}\exp(-\frac{(x-μ)^2}{2σ^2})\frac{(x-μ)}{\sigma}\sigma dx + μ$
これにより、変換ができるので、変換を行う。
$E[z] = \frac{1}{\sqrt(2π)}\int_{-∞}^{∞}\exp(-\frac{z^2}{2})zdz + μ$
ここで、部分積分を行う
$E[z] = -\frac{1}{\sqrt(2π\sigma^2)}\int_{-∞}^{∞}\exp(-\frac{z^2}{2})^′dz + μ$
計算すると、以下の式となる。
$E[z] = -\frac{1}{\sqrt(2π\sigma^2)}[\exp(-\frac{z^2}{2})]^{\infty}_{-\infty} + 
μ$
ここで、右辺の第一項は0となるから、期待値は$μ$となる。


ガウス分布の分散はどうなる？
まず、二次のモーメントを算出した後で、$V[x] = E[x^2] - E[x]^2$を使用して求める作戦を取る。
$E[x^2] = \int_{-∞}^{∞}\frac{1}{\sqrt(2πσ^2)}\exp(-\frac{(x-μ)^2}{2σ^2})x^2dx$と記載
ここで、以下のように変形する。
$E[x^2] = \int_{-∞}^{∞}\frac{1}{\sqrt(2πσ^2)}\exp(-\frac{(x-μ)^2}{2σ^2})((x-μ)^2 + 2xμ-μ^2)dx$－①
ここで、$\frac{(x-μ)}{\sigma}=z$と置き、両辺の$x$についての微分を取ると$\frac{1}{\sigma}dx = dz$となる。
①に代入すると、以下の式となる。
$E[z^2] = \int_{-∞}^{∞}\frac{1}{\sqrt(2πσ^2)}\exp(-\frac{z^2}{2})(z\sigma)^2\sigma dz + 2μ^2-μ^2$
整理すると、
$E[z^2] = \frac{\sigma^2}{\sqrt(2π)}\int_{-∞}^{∞}\exp(-\frac{z^2}{2})z^2 dz + 2μ^2-μ^2$
同じく部分積分を行うと、最終的に以下の式となる。
$E[x^2] = \sigma^2 + \mu^2$
従って、分散は
$V[x] = \sigma^2 + \mu^2 - \mu^2$より、$V[x] = \sigma^2$


